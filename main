from random import random, choice, randint
import pickle   #talvez eu use talvez não, pensando ainda
import matplotlib.pyplot as plt
import numpy as np


class Trash:
    def __init__(self, x, y) -> None:
        self.x = x
        self.y = y

    def __eq__(self, value) -> bool:
        return (value.x, value.y) == (self.x, self.y)


class Wall:
    def __init__(self, x, y) -> None:
        self.x = x
        self.y = y

    def __eq__(self, value) -> bool:
        return (value.x, value.y) == (self.x, self.y)


class Environment:
    def __init__(self, w=8, h=8, stumble=0, num_trash: int=3):
        self.w = w
        self.h = h
        self.stumble = stumble
        self.num_trash = num_trash
        self.pos_trash = {}
        self.inventory = []
        self.reset()

    def reset(self):
        self.pos = [self.h-1, 0]
        self.pos_trash = self.random_position_trash()
        return self.pos.copy()

    # generates random positions for garbage
    def random_position_trash(self):
        positions = set()

        while (len(positions) < self.num_trash):
            x = randint(0, self.h - 1)
            y = randint(0, self.w - 1)
            if (x, y) != tuple(self.pos) and (x, y) != (self.w-1, 0):
                positions.add((x, y))
        return {pos: f"trash{idx+1}" for idx, pos in enumerate(positions)}

    def action(self, act):
        if act == 0:  # Up
            self.pos[0] -= 1
        elif act == 1:  # Right
            self.pos[1] += 1
        elif act == 2:  # Down
            self.pos[0] += 1
        elif act == 3:  # Left
            self.pos[1] -= 1
        else:
            print("ERROR: Invalid action " + str(action))

        # Stumble
        if random() < self.stumble:
            self.pos[0] += choice([-1, 0, 1])
            self.pos[1] += choice([-1, 0, 1])

        # walls in environment
        self.pos[0] = min(self.h-1, max(0, self.pos[0]))
        self.pos[1] = min(self.w-1, max(0, self.pos[1]))

        score = 0
        final = False

        # location of the dump
        if self.pos[1] == self.w-1 and self.pos[0] == 0:
            score = 100
            final = True

        # positions walls
        elif self.pos[1] == self.w-2 and self.pos[0] == 5:
            final = True
        elif self.pos[1] == self.w-3 and self.pos[0] == 2:
            score = -100
            final = True
        elif self.pos[1] == self.w-5 and self.pos[0] == self.h-1:
            final = True
        elif self.pos[1] == self.w-6 and self.pos[0] == 3:
            score = -100
            final = True
        elif self.pos[1] == 0 and self.pos[0] == 0:
            score = -100
            final = True

        return self.pos.copy(), score, final

    def pick(self):
        # action of picking up an trash in the environment
        pos_tuple = tuple(self.pos)
        if pos_tuple in self.pos_trash:
            # remove the trash in the environment
            trash = self.pos_trash.pop(pos_tuple)
            self.inventory.append(trash)  # add to inventory
            return f"you caught {trash}!"
        else:
            return "you didn't catch anything"

    def check_victory(self):
        # Checks if the agent has reached the winning position and has 3 trahs
        if tuple(self.pos) == (self.w-1, 0):
            if len(self.inventory) == self.num_trash:
                return "you win", True
            else:
                return "You need more trash to win!", False
        return "keep playing", False

    def render_graphic(self):
        grid = np.zeros((self.h, self.w))

        # add walls
        walls = [(self.h-1, self.w-2), (2, self.w-3),
                 (self.h-1, self.w-5), (3, self.w-6), (0, 0)]
        for x, y in walls:
            grid[x][y] = 1  # Color one of the walls

        # add dump
        grid[0][self.w-1] = 3  # Color three from the landfill

        # add trash
        for (x, y), trash in self.pos_trash.items():
            grid[x][y] = 2  # Color two from lixos

        # Add agente
        grid[self.pos[0]][self.pos[1]] = 4  # Color forth of the agente

        # plot
        plt.imshow(grid, cmap="tab10")
        plt.colorbar(ticks=[0, 1, 2, 3, 4], label="Elements")
        plt.xticks(range(self.w))
        plt.yticks(range(self.h))
        plt.grid(True, color="black", linewidth=0.5)
        plt.title("visualization of the environment")
        plt.show()


class QLearning:
    def __init__(self, epsilon=0.1, alpha=0.1, gamma=0.9, init=0, w=4, h=4, a=4):
        self.epsilon = epsilon
        self.alpha = alpha
        self.gamma = gamma
        self.w = w
        self.h = h
        self.a = a
        self.init = init
        self.reset()

    def reset(self):
        self.qtable = [
            [[self.init] * self.a for j in range(self.w)] for i in range(self.h)]

    def printQTable(self):
        print("Pos\t|\tUp\t|\tRight\t|\tDown\t|\tLeft\t|")
        for i in range(self.h):
            for j in range(self.w):
                print("%d,%d\t|\t%d\t|\t%d\t|\t%d\t|\t%d\t|" % (
                    i, j, self.qtable[i][j][0], self.qtable[i][j][1], self.qtable[i][j][2], self.qtable[i][j][3]))

    def printPolicy(self):
        print("----" * self.w + "-")
        for i in range(self.h):
            print("|", end="")
            for j in range(self.w):
                best = self.getBestAction([i, j])
                print(" " + "↑→↓←"[best] + " |", end="")
            print("\n" + ("----" * self.w) + "-")

    def getMaxQ(self, pos):
        return max(self.qtable[pos[0]][pos[1]])

    def getBestAction(self, pos):
        qs = self.qtable[pos[0]][pos[1]]
        m = max(qs)
        bests = [i for i, j in enumerate(qs) if j == m]
        return choice(bests)

    def getRandomAction(self):
        return int(random()*self.a)

    def getAction(self, pos):
        if random() < self.epsilon:
            return self.getRandomAction()
        else:
            return self.getBestAction(pos)

    def update(self, oldpos, action, newpos, reward, final):
        if final:
            self.qtable[oldpos[0]][oldpos[1]][action] += self.alpha * \
                (reward - self.qtable[oldpos[0]][oldpos[1]][action])
        else:
            self.qtable[oldpos[0]][oldpos[1]][action] += self.alpha * (
                reward + self.gamma * self.getMaxQ(newpos) - self.qtable[oldpos[0]][oldpos[1]][action])


R = 8
C = 8
episodes = 10000
environment = Environment(w=C, h=R, stumble=0.1)
ql = QLearning(w=C, h=R, alpha=0.2, epsilon=0.5, init=100)
for i in range(episodes):
    # prevents the robot from exploring too much at the end of the simulations
    ql.epsilon = max(0.1, ql.epsilon * 0.99)
    state = environment.reset()
    environment.render_graphic()
    for step in range(R * C):
        action = ql.getAction(state)
        newstate, reward, final = environment.action(action)
        # if tuple(state) in environment.pos_trash:
        #     print(environment.pick())
        #     reward += 300
        print(environment.pick())
        if tuple(state) in environment.pos_trash:
            reward += 300

        ql.update(state, action, newstate, reward, final)
        state = newstate
        environment.render_graphic()
        if final:
            break
# ql.printQTable()
# ql.printPolicy()
